{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stockprediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5dy1uVrRBsGQOkYtoVzcr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alteregoxiv/stockpred/blob/main/stockprediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgd1YiGWppPz"
      },
      "source": [
        "# **STOCK PREDICTION USING LSTM(KERAS)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev5-W2HvWUpB"
      },
      "source": [
        "# **Importing necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT7zcAoIcksy"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 402,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVYQZhNWcKe"
      },
      "source": [
        "# **Training Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zxg_MYh3SOAk",
        "outputId": "e316c42a-5d7c-4a04-9921-8398d04a8a6b"
      },
      "source": [
        "traindata = pd.read_csv(\"./AMZNtrain.csv\")\n",
        "traindata.head()"
      ],
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014-01-02</td>\n",
              "      <td>398.799988</td>\n",
              "      <td>399.359985</td>\n",
              "      <td>394.019989</td>\n",
              "      <td>397.970001</td>\n",
              "      <td>397.970001</td>\n",
              "      <td>2137800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014-01-03</td>\n",
              "      <td>398.290009</td>\n",
              "      <td>402.709991</td>\n",
              "      <td>396.220001</td>\n",
              "      <td>396.440002</td>\n",
              "      <td>396.440002</td>\n",
              "      <td>2210200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014-01-06</td>\n",
              "      <td>395.850006</td>\n",
              "      <td>397.000000</td>\n",
              "      <td>388.420013</td>\n",
              "      <td>393.630005</td>\n",
              "      <td>393.630005</td>\n",
              "      <td>3170600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2014-01-07</td>\n",
              "      <td>395.040009</td>\n",
              "      <td>398.470001</td>\n",
              "      <td>394.290009</td>\n",
              "      <td>398.029999</td>\n",
              "      <td>398.029999</td>\n",
              "      <td>1916000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014-01-08</td>\n",
              "      <td>398.470001</td>\n",
              "      <td>403.000000</td>\n",
              "      <td>396.040009</td>\n",
              "      <td>401.920013</td>\n",
              "      <td>401.920013</td>\n",
              "      <td>2316500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date        Open        High  ...       Close   Adj Close   Volume\n",
              "0  2014-01-02  398.799988  399.359985  ...  397.970001  397.970001  2137800\n",
              "1  2014-01-03  398.290009  402.709991  ...  396.440002  396.440002  2210200\n",
              "2  2014-01-06  395.850006  397.000000  ...  393.630005  393.630005  3170600\n",
              "3  2014-01-07  395.040009  398.470001  ...  398.029999  398.029999  1916000\n",
              "4  2014-01-08  398.470001  403.000000  ...  401.920013  401.920013  2316500\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 403
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPEpZlynWnQW"
      },
      "source": [
        "# **Testing Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JVYD_bNkSg5d",
        "outputId": "1b803c46-ebea-421c-d298-b0a21923f823"
      },
      "source": [
        "testdata = pd.read_csv(\"./AMZNtest.csv\")\n",
        "testdata.head()"
      ],
      "execution_count": 404,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-01-02</td>\n",
              "      <td>1465.199951</td>\n",
              "      <td>1553.359985</td>\n",
              "      <td>1460.930054</td>\n",
              "      <td>1539.130005</td>\n",
              "      <td>1539.130005</td>\n",
              "      <td>7983100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-01-03</td>\n",
              "      <td>1520.010010</td>\n",
              "      <td>1538.000000</td>\n",
              "      <td>1497.109985</td>\n",
              "      <td>1500.280029</td>\n",
              "      <td>1500.280029</td>\n",
              "      <td>6975600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-04</td>\n",
              "      <td>1530.000000</td>\n",
              "      <td>1594.000000</td>\n",
              "      <td>1518.310059</td>\n",
              "      <td>1575.390015</td>\n",
              "      <td>1575.390015</td>\n",
              "      <td>9182600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-01-07</td>\n",
              "      <td>1602.310059</td>\n",
              "      <td>1634.560059</td>\n",
              "      <td>1589.189941</td>\n",
              "      <td>1629.510010</td>\n",
              "      <td>1629.510010</td>\n",
              "      <td>7993200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-01-08</td>\n",
              "      <td>1664.689941</td>\n",
              "      <td>1676.609985</td>\n",
              "      <td>1616.609985</td>\n",
              "      <td>1656.579956</td>\n",
              "      <td>1656.579956</td>\n",
              "      <td>8881400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date         Open         High  ...        Close    Adj Close   Volume\n",
              "0  2019-01-02  1465.199951  1553.359985  ...  1539.130005  1539.130005  7983100\n",
              "1  2019-01-03  1520.010010  1538.000000  ...  1500.280029  1500.280029  6975600\n",
              "2  2019-01-04  1530.000000  1594.000000  ...  1575.390015  1575.390015  9182600\n",
              "3  2019-01-07  1602.310059  1634.560059  ...  1629.510010  1629.510010  7993200\n",
              "4  2019-01-08  1664.689941  1676.609985  ...  1656.579956  1656.579956  8881400\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 404
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URHQm4XdWrNU"
      },
      "source": [
        "# **Shapes of both data sets(Train , Test)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF6nBNrZU7Ux",
        "outputId": "1abc6ea1-35f7-4fcb-cdbd-05ef3373c19b"
      },
      "source": [
        "traindata.shape , testdata.shape"
      ],
      "execution_count": 405,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1258, 7), (21, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 405
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZOISlTXUqo"
      },
      "source": [
        "# **Concatenating Datasers and Sorting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMCJ0YqEVVlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ed3bc9-0b36-4e58-cde2-b457902ebc30"
      },
      "source": [
        "data = pd.concat([traindata , testdata] , axis=0 , ignore_index=True)\n",
        "data.head"
      ],
      "execution_count": 406,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of             Date         Open         High  ...        Close    Adj Close    Volume\n",
              "0     2014-01-02   398.799988   399.359985  ...   397.970001   397.970001   2137800\n",
              "1     2014-01-03   398.290009   402.709991  ...   396.440002   396.440002   2210200\n",
              "2     2014-01-06   395.850006   397.000000  ...   393.630005   393.630005   3170600\n",
              "3     2014-01-07   395.040009   398.470001  ...   398.029999   398.029999   1916000\n",
              "4     2014-01-08   398.470001   403.000000  ...   401.920013   401.920013   2316500\n",
              "...          ...          ...          ...  ...          ...          ...       ...\n",
              "1274  2019-01-25  1670.500000  1683.479980  ...  1670.569946  1670.569946   4945900\n",
              "1275  2019-01-28  1643.589966  1645.000000  ...  1637.890015  1637.890015   4837700\n",
              "1276  2019-01-29  1631.270020  1632.380005  ...  1593.880005  1593.880005   4632800\n",
              "1277  2019-01-30  1623.000000  1676.949951  ...  1670.430054  1670.430054   5783800\n",
              "1278  2019-01-31  1692.849976  1736.410034  ...  1718.729980  1718.729980  10910300\n",
              "\n",
              "[1279 rows x 7 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 406
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "SEFBWlNzXazd",
        "outputId": "909b1e3c-0993-4175-b6d3-0803ca25401b"
      },
      "source": [
        "for i in range(len(data)):\n",
        "  date = data[\"Date\"][i]\n",
        "  d = date.split(\"-\")\n",
        "  s = str(d[0]) + str(d[1]) + str(d[2])\n",
        "  data[\"Date\"][i] = s\n",
        "data.head()"
      ],
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20140102</td>\n",
              "      <td>398.799988</td>\n",
              "      <td>399.359985</td>\n",
              "      <td>394.019989</td>\n",
              "      <td>397.970001</td>\n",
              "      <td>397.970001</td>\n",
              "      <td>2137800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20140103</td>\n",
              "      <td>398.290009</td>\n",
              "      <td>402.709991</td>\n",
              "      <td>396.220001</td>\n",
              "      <td>396.440002</td>\n",
              "      <td>396.440002</td>\n",
              "      <td>2210200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20140106</td>\n",
              "      <td>395.850006</td>\n",
              "      <td>397.000000</td>\n",
              "      <td>388.420013</td>\n",
              "      <td>393.630005</td>\n",
              "      <td>393.630005</td>\n",
              "      <td>3170600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20140107</td>\n",
              "      <td>395.040009</td>\n",
              "      <td>398.470001</td>\n",
              "      <td>394.290009</td>\n",
              "      <td>398.029999</td>\n",
              "      <td>398.029999</td>\n",
              "      <td>1916000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20140108</td>\n",
              "      <td>398.470001</td>\n",
              "      <td>403.000000</td>\n",
              "      <td>396.040009</td>\n",
              "      <td>401.920013</td>\n",
              "      <td>401.920013</td>\n",
              "      <td>2316500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Date        Open        High  ...       Close   Adj Close   Volume\n",
              "0  20140102  398.799988  399.359985  ...  397.970001  397.970001  2137800\n",
              "1  20140103  398.290009  402.709991  ...  396.440002  396.440002  2210200\n",
              "2  20140106  395.850006  397.000000  ...  393.630005  393.630005  3170600\n",
              "3  20140107  395.040009  398.470001  ...  398.029999  398.029999  1916000\n",
              "4  20140108  398.470001  403.000000  ...  401.920013  401.920013  2316500\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 407
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl95h11OhUXG",
        "outputId": "10c7a2bf-d73c-41e2-d47e-6107688d3a56"
      },
      "source": [
        "data = data.sort_index(ascending=True , axis=0)\n",
        "data.head"
      ],
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of           Date         Open         High  ...        Close    Adj Close    Volume\n",
              "0     20140102   398.799988   399.359985  ...   397.970001   397.970001   2137800\n",
              "1     20140103   398.290009   402.709991  ...   396.440002   396.440002   2210200\n",
              "2     20140106   395.850006   397.000000  ...   393.630005   393.630005   3170600\n",
              "3     20140107   395.040009   398.470001  ...   398.029999   398.029999   1916000\n",
              "4     20140108   398.470001   403.000000  ...   401.920013   401.920013   2316500\n",
              "...        ...          ...          ...  ...          ...          ...       ...\n",
              "1274  20190125  1670.500000  1683.479980  ...  1670.569946  1670.569946   4945900\n",
              "1275  20190128  1643.589966  1645.000000  ...  1637.890015  1637.890015   4837700\n",
              "1276  20190129  1631.270020  1632.380005  ...  1593.880005  1593.880005   4632800\n",
              "1277  20190130  1623.000000  1676.949951  ...  1670.430054  1670.430054   5783800\n",
              "1278  20190131  1692.849976  1736.410034  ...  1718.729980  1718.729980  10910300\n",
              "\n",
              "[1279 rows x 7 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 408
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmL2H46GhxzR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFvm1e-Vnsvt"
      },
      "source": [
        "# **Saving the DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBruMR6WhxB7"
      },
      "source": [
        "data.to_csv(r'./AMZNTotalData.csv' , index=False)"
      ],
      "execution_count": 409,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvQ5cI7ykyvr",
        "outputId": "08341e0f-2c8c-4147-8758-994445564558"
      },
      "source": [
        "newdata = pd.read_csv(\"./AMZNTotalData.csv\")\n",
        "newdata.head"
      ],
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of           Date         Open         High  ...        Close    Adj Close    Volume\n",
              "0     20140102   398.799988   399.359985  ...   397.970001   397.970001   2137800\n",
              "1     20140103   398.290009   402.709991  ...   396.440002   396.440002   2210200\n",
              "2     20140106   395.850006   397.000000  ...   393.630005   393.630005   3170600\n",
              "3     20140107   395.040009   398.470001  ...   398.029999   398.029999   1916000\n",
              "4     20140108   398.470001   403.000000  ...   401.920013   401.920013   2316500\n",
              "...        ...          ...          ...  ...          ...          ...       ...\n",
              "1274  20190125  1670.500000  1683.479980  ...  1670.569946  1670.569946   4945900\n",
              "1275  20190128  1643.589966  1645.000000  ...  1637.890015  1637.890015   4837700\n",
              "1276  20190129  1631.270020  1632.380005  ...  1593.880005  1593.880005   4632800\n",
              "1277  20190130  1623.000000  1676.949951  ...  1670.430054  1670.430054   5783800\n",
              "1278  20190131  1692.849976  1736.410034  ...  1718.729980  1718.729980  10910300\n",
              "\n",
              "[1279 rows x 7 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 410
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37LGB3wEp93X"
      },
      "source": [
        "# **Test Train Split from the concatenated Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZnXpPuIonv9",
        "outputId": "811f3c82-b01d-4d2f-ba56-4044a078a521"
      },
      "source": [
        "limit = np.random.rand((len(newdata)))<0.7\n",
        "train = newdata[limit]\n",
        "test = newdata[~limit]\n",
        "train.shape , test.shape"
      ],
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((851, 7), (428, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 411
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXTXV-D4qQot",
        "outputId": "317f754e-b569-4fa8-dc50-5cc81592e9d4"
      },
      "source": [
        "newtrain = train[['Adj Close']]\n",
        "newtest = test[['Adj Close']]\n",
        "\n",
        "newtrain.shape , newtest.shape"
      ],
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((851, 1), (428, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 412
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD7YgMi7yU3g"
      },
      "source": [
        "# **Scaling the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5e02dL5-PoF"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "newtrain = sc.fit_transform(newtrain)\n",
        "newtest = sc.fit_transform(newtest)\n",
        "\n",
        "newtrain = np.reshape(newtrain , (-1 , 1))\n",
        "newtest = np.reshape(newtest , (-1 , 1))"
      ],
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N84K9DW2yepY"
      },
      "source": [
        "# **Preparing the Final data using timestep for LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAixTgSxeNNF"
      },
      "source": [
        "def final_data(data , timestep):\n",
        "  xdata , ydata = [] , []\n",
        "  l = len(data)\n",
        "  for i in range(l-timestep):\n",
        "    a = data[i : i+timestep , 0]\n",
        "    xdata.append(a)\n",
        "    ydata.append(data[i+timestep , 0])\n",
        "  return np.array(xdata) , np.array(ydata)\n",
        "\n",
        "\n",
        "\n",
        "timestep = 60\n",
        "xtrain  , ytrain = final_data(newtrain , timestep)\n",
        "xtest , ytest = final_data(newtest , timestep)"
      ],
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1JwMQYh5GZ"
      },
      "source": [
        "xtrain = xtrain.reshape(xtrain.shape[0] , xtrain.shape[1] , 1)\n",
        "xtest = xtest.reshape(xtest.shape[0] , xtest.shape[1] , 1)\n",
        "#xtrain.shape\n",
        "#print(xtrain)"
      ],
      "execution_count": 415,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G-BglZkzh6V"
      },
      "source": [
        "# **Preparing Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC_0T-jFjw6E"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , LSTM"
      ],
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra1u6nyUkkVV"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50 , return_sequences=True , input_shape=(60 , 1)))\n",
        "model.add(LSTM(50 , return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error' , optimizer='adam')"
      ],
      "execution_count": 417,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWzEihXDnLmu",
        "outputId": "37ce3315-97ea-45d9-fbb0-e6cab71a28f8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_50 (LSTM)               (None, 60, 50)            10400     \n",
            "_________________________________________________________________\n",
            "lstm_51 (LSTM)               (None, 60, 50)            20200     \n",
            "_________________________________________________________________\n",
            "lstm_52 (LSTM)               (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 50,851\n",
            "Trainable params: 50,851\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GUCkn8pzmYy"
      },
      "source": [
        "# **Fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IESruTGgnRog",
        "outputId": "55b02a53-4ed0-48ed-ec63-a54d58a00073"
      },
      "source": [
        "model.fit(xtrain , ytrain , validation_data=(xtest , ytest) , epochs=300 , batch_size=64 , verbose=1)"
      ],
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "13/13 [==============================] - 7s 197ms/step - loss: 0.0589 - val_loss: 0.0109\n",
            "Epoch 2/300\n",
            "13/13 [==============================] - 1s 112ms/step - loss: 0.0082 - val_loss: 0.0054\n",
            "Epoch 3/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 0.0025 - val_loss: 0.0033\n",
            "Epoch 4/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 0.0015 - val_loss: 0.0029\n",
            "Epoch 5/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 6/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 7/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 0.0013 - val_loss: 0.0026\n",
            "Epoch 8/300\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 9/300\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 10/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 11/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 12/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 0.0013 - val_loss: 0.0032\n",
            "Epoch 13/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 0.0015 - val_loss: 0.0031\n",
            "Epoch 14/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 15/300\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 16/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 0.0010 - val_loss: 0.0022\n",
            "Epoch 17/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 18/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 19/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 20/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 21/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0010 - val_loss: 0.0023\n",
            "Epoch 22/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 0.0010 - val_loss: 0.0018\n",
            "Epoch 23/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 9.2668e-04 - val_loss: 0.0018\n",
            "Epoch 24/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 25/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 9.2337e-04 - val_loss: 0.0016\n",
            "Epoch 26/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 9.0994e-04 - val_loss: 0.0016\n",
            "Epoch 27/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 8.9739e-04 - val_loss: 0.0016\n",
            "Epoch 28/300\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 8.4184e-04 - val_loss: 0.0015\n",
            "Epoch 29/300\n",
            "13/13 [==============================] - 2s 116ms/step - loss: 8.8405e-04 - val_loss: 0.0014\n",
            "Epoch 30/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 8.9766e-04 - val_loss: 0.0015\n",
            "Epoch 31/300\n",
            "13/13 [==============================] - 2s 130ms/step - loss: 8.2912e-04 - val_loss: 0.0015\n",
            "Epoch 32/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 7.9008e-04 - val_loss: 0.0013\n",
            "Epoch 33/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 7.5380e-04 - val_loss: 0.0012\n",
            "Epoch 34/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 7.4609e-04 - val_loss: 0.0012\n",
            "Epoch 35/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 8.1865e-04 - val_loss: 0.0013\n",
            "Epoch 36/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 7.3987e-04 - val_loss: 0.0012\n",
            "Epoch 37/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 7.2404e-04 - val_loss: 0.0014\n",
            "Epoch 38/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 7.5954e-04 - val_loss: 0.0012\n",
            "Epoch 39/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 8.1401e-04 - val_loss: 0.0013\n",
            "Epoch 40/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 8.9785e-04 - val_loss: 0.0017\n",
            "Epoch 41/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 8.4973e-04 - val_loss: 0.0012\n",
            "Epoch 42/300\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 7.4544e-04 - val_loss: 0.0012\n",
            "Epoch 43/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 7.7782e-04 - val_loss: 0.0014\n",
            "Epoch 44/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 6.8214e-04 - val_loss: 0.0011\n",
            "Epoch 45/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 6.7113e-04 - val_loss: 0.0011\n",
            "Epoch 46/300\n",
            "13/13 [==============================] - 2s 116ms/step - loss: 8.7345e-04 - val_loss: 0.0016\n",
            "Epoch 47/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 7.6654e-04 - val_loss: 0.0012\n",
            "Epoch 48/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 7.8345e-04 - val_loss: 0.0014\n",
            "Epoch 49/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 8.0153e-04 - val_loss: 0.0011\n",
            "Epoch 50/300\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 6.8157e-04 - val_loss: 0.0011\n",
            "Epoch 51/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 6.3706e-04 - val_loss: 9.9833e-04\n",
            "Epoch 52/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 6.6793e-04 - val_loss: 0.0012\n",
            "Epoch 53/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 7.1160e-04 - val_loss: 0.0010\n",
            "Epoch 54/300\n",
            "13/13 [==============================] - 2s 116ms/step - loss: 6.2209e-04 - val_loss: 0.0010\n",
            "Epoch 55/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 6.7518e-04 - val_loss: 0.0010\n",
            "Epoch 56/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 6.0786e-04 - val_loss: 9.4892e-04\n",
            "Epoch 57/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 6.0746e-04 - val_loss: 9.9389e-04\n",
            "Epoch 58/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 6.9151e-04 - val_loss: 0.0014\n",
            "Epoch 59/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 6.8881e-04 - val_loss: 0.0010\n",
            "Epoch 60/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 7.4236e-04 - val_loss: 9.6263e-04\n",
            "Epoch 61/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 5.9261e-04 - val_loss: 9.8377e-04\n",
            "Epoch 62/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 5.7362e-04 - val_loss: 0.0010\n",
            "Epoch 63/300\n",
            "13/13 [==============================] - 2s 128ms/step - loss: 5.7628e-04 - val_loss: 9.2379e-04\n",
            "Epoch 64/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 5.7125e-04 - val_loss: 8.7680e-04\n",
            "Epoch 65/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 5.8736e-04 - val_loss: 0.0011\n",
            "Epoch 66/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 5.8991e-04 - val_loss: 9.3429e-04\n",
            "Epoch 67/300\n",
            "13/13 [==============================] - 1s 113ms/step - loss: 7.4167e-04 - val_loss: 0.0017\n",
            "Epoch 68/300\n",
            "13/13 [==============================] - 2s 130ms/step - loss: 6.6723e-04 - val_loss: 8.9910e-04\n",
            "Epoch 69/300\n",
            "13/13 [==============================] - 2s 140ms/step - loss: 5.4264e-04 - val_loss: 8.7583e-04\n",
            "Epoch 70/300\n",
            "13/13 [==============================] - 2s 128ms/step - loss: 5.4931e-04 - val_loss: 9.0067e-04\n",
            "Epoch 71/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 5.2149e-04 - val_loss: 9.1317e-04\n",
            "Epoch 72/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 5.2190e-04 - val_loss: 8.3708e-04\n",
            "Epoch 73/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 5.2272e-04 - val_loss: 9.4054e-04\n",
            "Epoch 74/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 4.9572e-04 - val_loss: 8.8980e-04\n",
            "Epoch 75/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 6.5113e-04 - val_loss: 8.4793e-04\n",
            "Epoch 76/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 7.1071e-04 - val_loss: 9.3815e-04\n",
            "Epoch 77/300\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 5.7379e-04 - val_loss: 9.6200e-04\n",
            "Epoch 78/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 7.3703e-04 - val_loss: 9.2538e-04\n",
            "Epoch 79/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 6.1165e-04 - val_loss: 8.0577e-04\n",
            "Epoch 80/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 4.6044e-04 - val_loss: 8.6366e-04\n",
            "Epoch 81/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 4.5581e-04 - val_loss: 9.0904e-04\n",
            "Epoch 82/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 4.6378e-04 - val_loss: 8.1738e-04\n",
            "Epoch 83/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 4.3944e-04 - val_loss: 8.4581e-04\n",
            "Epoch 84/300\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 4.2585e-04 - val_loss: 7.9905e-04\n",
            "Epoch 85/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 4.4191e-04 - val_loss: 7.6262e-04\n",
            "Epoch 86/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 4.9056e-04 - val_loss: 8.0426e-04\n",
            "Epoch 87/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 5.0980e-04 - val_loss: 0.0015\n",
            "Epoch 88/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 6.1881e-04 - val_loss: 0.0012\n",
            "Epoch 89/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 6.1181e-04 - val_loss: 7.6019e-04\n",
            "Epoch 90/300\n",
            "13/13 [==============================] - 2s 130ms/step - loss: 4.4051e-04 - val_loss: 8.7109e-04\n",
            "Epoch 91/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 4.0980e-04 - val_loss: 8.1017e-04\n",
            "Epoch 92/300\n",
            "13/13 [==============================] - 1s 117ms/step - loss: 4.3020e-04 - val_loss: 9.6985e-04\n",
            "Epoch 93/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.9363e-04 - val_loss: 8.6159e-04\n",
            "Epoch 94/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 4.5627e-04 - val_loss: 0.0013\n",
            "Epoch 95/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 5.9788e-04 - val_loss: 8.0574e-04\n",
            "Epoch 96/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 5.5383e-04 - val_loss: 7.6187e-04\n",
            "Epoch 97/300\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 5.7666e-04 - val_loss: 9.2641e-04\n",
            "Epoch 98/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 4.7586e-04 - val_loss: 7.6145e-04\n",
            "Epoch 99/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 3.8569e-04 - val_loss: 7.8825e-04\n",
            "Epoch 100/300\n",
            "13/13 [==============================] - 2s 116ms/step - loss: 4.1302e-04 - val_loss: 7.9719e-04\n",
            "Epoch 101/300\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 3.6212e-04 - val_loss: 7.7011e-04\n",
            "Epoch 102/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 3.9946e-04 - val_loss: 8.3858e-04\n",
            "Epoch 103/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 3.7389e-04 - val_loss: 7.5987e-04\n",
            "Epoch 104/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.7413e-04 - val_loss: 7.8137e-04\n",
            "Epoch 105/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 3.6502e-04 - val_loss: 7.1611e-04\n",
            "Epoch 106/300\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 3.7832e-04 - val_loss: 7.4329e-04\n",
            "Epoch 107/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.6755e-04 - val_loss: 8.3262e-04\n",
            "Epoch 108/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 4.3104e-04 - val_loss: 7.2326e-04\n",
            "Epoch 109/300\n",
            "13/13 [==============================] - 2s 136ms/step - loss: 4.8551e-04 - val_loss: 7.9663e-04\n",
            "Epoch 110/300\n",
            "13/13 [==============================] - 2s 137ms/step - loss: 3.6215e-04 - val_loss: 7.2966e-04\n",
            "Epoch 111/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.5019e-04 - val_loss: 7.0485e-04\n",
            "Epoch 112/300\n",
            "13/13 [==============================] - 2s 138ms/step - loss: 3.8247e-04 - val_loss: 7.2109e-04\n",
            "Epoch 113/300\n",
            "13/13 [==============================] - 2s 126ms/step - loss: 3.5597e-04 - val_loss: 7.4400e-04\n",
            "Epoch 114/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 3.4647e-04 - val_loss: 8.4181e-04\n",
            "Epoch 115/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.3883e-04 - val_loss: 7.1976e-04\n",
            "Epoch 116/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 3.3528e-04 - val_loss: 7.1680e-04\n",
            "Epoch 117/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.5167e-04 - val_loss: 7.5243e-04\n",
            "Epoch 118/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 3.7886e-04 - val_loss: 6.9606e-04\n",
            "Epoch 119/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 4.1845e-04 - val_loss: 8.6428e-04\n",
            "Epoch 120/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.8254e-04 - val_loss: 9.3798e-04\n",
            "Epoch 121/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 3.4060e-04 - val_loss: 7.5857e-04\n",
            "Epoch 122/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.1675e-04 - val_loss: 7.3144e-04\n",
            "Epoch 123/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.1746e-04 - val_loss: 7.1929e-04\n",
            "Epoch 124/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.6032e-04 - val_loss: 6.7765e-04\n",
            "Epoch 125/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 3.3084e-04 - val_loss: 7.3155e-04\n",
            "Epoch 126/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.6788e-04 - val_loss: 7.1119e-04\n",
            "Epoch 127/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 3.0884e-04 - val_loss: 8.2842e-04\n",
            "Epoch 128/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 3.2689e-04 - val_loss: 7.3790e-04\n",
            "Epoch 129/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.0809e-04 - val_loss: 7.9305e-04\n",
            "Epoch 130/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.3371e-04 - val_loss: 6.9411e-04\n",
            "Epoch 131/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.9723e-04 - val_loss: 7.5925e-04\n",
            "Epoch 132/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.1036e-04 - val_loss: 9.9736e-04\n",
            "Epoch 133/300\n",
            "13/13 [==============================] - 1s 116ms/step - loss: 4.6039e-04 - val_loss: 7.6110e-04\n",
            "Epoch 134/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 3.2222e-04 - val_loss: 7.1793e-04\n",
            "Epoch 135/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 3.7085e-04 - val_loss: 0.0010\n",
            "Epoch 136/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 4.5185e-04 - val_loss: 7.6783e-04\n",
            "Epoch 137/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 3.3775e-04 - val_loss: 6.6056e-04\n",
            "Epoch 138/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 3.1413e-04 - val_loss: 6.6565e-04\n",
            "Epoch 139/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.6251e-04 - val_loss: 7.3486e-04\n",
            "Epoch 140/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 3.1358e-04 - val_loss: 6.8129e-04\n",
            "Epoch 141/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.9516e-04 - val_loss: 7.0892e-04\n",
            "Epoch 142/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.9499e-04 - val_loss: 6.4867e-04\n",
            "Epoch 143/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.3355e-04 - val_loss: 7.6131e-04\n",
            "Epoch 144/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 3.2259e-04 - val_loss: 7.4427e-04\n",
            "Epoch 145/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.9459e-04 - val_loss: 6.4273e-04\n",
            "Epoch 146/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 2.7759e-04 - val_loss: 7.1078e-04\n",
            "Epoch 147/300\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 2.7738e-04 - val_loss: 7.3337e-04\n",
            "Epoch 148/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 2.9801e-04 - val_loss: 6.1820e-04\n",
            "Epoch 149/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.7159e-04 - val_loss: 6.1876e-04\n",
            "Epoch 150/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.7176e-04 - val_loss: 6.5518e-04\n",
            "Epoch 151/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 3.2912e-04 - val_loss: 8.0732e-04\n",
            "Epoch 152/300\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 3.0394e-04 - val_loss: 7.0726e-04\n",
            "Epoch 153/300\n",
            "13/13 [==============================] - 1s 117ms/step - loss: 2.8679e-04 - val_loss: 6.8252e-04\n",
            "Epoch 154/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 2.6246e-04 - val_loss: 6.3049e-04\n",
            "Epoch 155/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 3.2546e-04 - val_loss: 5.9689e-04\n",
            "Epoch 156/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.7512e-04 - val_loss: 8.2764e-04\n",
            "Epoch 157/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 3.1884e-04 - val_loss: 6.7624e-04\n",
            "Epoch 158/300\n",
            "13/13 [==============================] - 2s 132ms/step - loss: 3.1040e-04 - val_loss: 6.7812e-04\n",
            "Epoch 159/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 3.0214e-04 - val_loss: 6.8743e-04\n",
            "Epoch 160/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.8251e-04 - val_loss: 6.3240e-04\n",
            "Epoch 161/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 3.0202e-04 - val_loss: 6.5717e-04\n",
            "Epoch 162/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.3319e-04 - val_loss: 7.5045e-04\n",
            "Epoch 163/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 3.8151e-04 - val_loss: 5.9537e-04\n",
            "Epoch 164/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 3.4993e-04 - val_loss: 6.0373e-04\n",
            "Epoch 165/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 2.9191e-04 - val_loss: 5.8373e-04\n",
            "Epoch 166/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 3.5742e-04 - val_loss: 6.1142e-04\n",
            "Epoch 167/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.6679e-04 - val_loss: 5.7026e-04\n",
            "Epoch 168/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 2.7374e-04 - val_loss: 5.8297e-04\n",
            "Epoch 169/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 2.5142e-04 - val_loss: 5.9990e-04\n",
            "Epoch 170/300\n",
            "13/13 [==============================] - 2s 134ms/step - loss: 2.5335e-04 - val_loss: 7.1434e-04\n",
            "Epoch 171/300\n",
            "13/13 [==============================] - 2s 140ms/step - loss: 2.8007e-04 - val_loss: 5.8893e-04\n",
            "Epoch 172/300\n",
            "13/13 [==============================] - 2s 139ms/step - loss: 2.7597e-04 - val_loss: 5.8929e-04\n",
            "Epoch 173/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.9386e-04 - val_loss: 5.7744e-04\n",
            "Epoch 174/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.4768e-04 - val_loss: 6.1371e-04\n",
            "Epoch 175/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.7375e-04 - val_loss: 5.4754e-04\n",
            "Epoch 176/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.6649e-04 - val_loss: 5.8256e-04\n",
            "Epoch 177/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.4677e-04 - val_loss: 5.9351e-04\n",
            "Epoch 178/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.3851e-04 - val_loss: 5.2961e-04\n",
            "Epoch 179/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 2.3989e-04 - val_loss: 5.6183e-04\n",
            "Epoch 180/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.3435e-04 - val_loss: 5.7492e-04\n",
            "Epoch 181/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.5243e-04 - val_loss: 5.4047e-04\n",
            "Epoch 182/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.7209e-04 - val_loss: 5.2603e-04\n",
            "Epoch 183/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.4744e-04 - val_loss: 5.3957e-04\n",
            "Epoch 184/300\n",
            "13/13 [==============================] - 2s 128ms/step - loss: 2.3300e-04 - val_loss: 5.3192e-04\n",
            "Epoch 185/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 2.3260e-04 - val_loss: 8.0179e-04\n",
            "Epoch 186/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 3.1871e-04 - val_loss: 6.3359e-04\n",
            "Epoch 187/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.5151e-04 - val_loss: 6.0647e-04\n",
            "Epoch 188/300\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 2.4047e-04 - val_loss: 5.1680e-04\n",
            "Epoch 189/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.6693e-04 - val_loss: 7.6026e-04\n",
            "Epoch 190/300\n",
            "13/13 [==============================] - 2s 132ms/step - loss: 2.6696e-04 - val_loss: 7.1771e-04\n",
            "Epoch 191/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 3.3189e-04 - val_loss: 5.4337e-04\n",
            "Epoch 192/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.8741e-04 - val_loss: 5.2928e-04\n",
            "Epoch 193/300\n",
            "13/13 [==============================] - 2s 126ms/step - loss: 2.3812e-04 - val_loss: 7.2810e-04\n",
            "Epoch 194/300\n",
            "13/13 [==============================] - 2s 129ms/step - loss: 3.1532e-04 - val_loss: 5.1325e-04\n",
            "Epoch 195/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.5431e-04 - val_loss: 5.3433e-04\n",
            "Epoch 196/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.8709e-04 - val_loss: 5.0245e-04\n",
            "Epoch 197/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.3687e-04 - val_loss: 5.0246e-04\n",
            "Epoch 198/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.1970e-04 - val_loss: 4.9990e-04\n",
            "Epoch 199/300\n",
            "13/13 [==============================] - 2s 134ms/step - loss: 2.2020e-04 - val_loss: 4.8076e-04\n",
            "Epoch 200/300\n",
            "13/13 [==============================] - 2s 127ms/step - loss: 2.2177e-04 - val_loss: 5.3354e-04\n",
            "Epoch 201/300\n",
            "13/13 [==============================] - 2s 139ms/step - loss: 2.4170e-04 - val_loss: 5.1113e-04\n",
            "Epoch 202/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.2336e-04 - val_loss: 5.2879e-04\n",
            "Epoch 203/300\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 2.1850e-04 - val_loss: 4.9211e-04\n",
            "Epoch 204/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.2620e-04 - val_loss: 6.1274e-04\n",
            "Epoch 205/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.5091e-04 - val_loss: 4.9417e-04\n",
            "Epoch 206/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.7279e-04 - val_loss: 5.1980e-04\n",
            "Epoch 207/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.2965e-04 - val_loss: 5.6424e-04\n",
            "Epoch 208/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 3.0887e-04 - val_loss: 5.2115e-04\n",
            "Epoch 209/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.6447e-04 - val_loss: 4.7225e-04\n",
            "Epoch 210/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.5049e-04 - val_loss: 5.2849e-04\n",
            "Epoch 211/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.4071e-04 - val_loss: 5.2490e-04\n",
            "Epoch 212/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.1493e-04 - val_loss: 5.0202e-04\n",
            "Epoch 213/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.2142e-04 - val_loss: 4.7096e-04\n",
            "Epoch 214/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.2064e-04 - val_loss: 4.9958e-04\n",
            "Epoch 215/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.2202e-04 - val_loss: 4.9278e-04\n",
            "Epoch 216/300\n",
            "13/13 [==============================] - 2s 126ms/step - loss: 2.1848e-04 - val_loss: 5.2772e-04\n",
            "Epoch 217/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 2.3513e-04 - val_loss: 6.0101e-04\n",
            "Epoch 218/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.9559e-04 - val_loss: 5.2369e-04\n",
            "Epoch 219/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.4985e-04 - val_loss: 6.0987e-04\n",
            "Epoch 220/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.6003e-04 - val_loss: 5.0836e-04\n",
            "Epoch 221/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.3487e-04 - val_loss: 6.3958e-04\n",
            "Epoch 222/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 2.5255e-04 - val_loss: 4.7402e-04\n",
            "Epoch 223/300\n",
            "13/13 [==============================] - 2s 126ms/step - loss: 2.2241e-04 - val_loss: 4.4107e-04\n",
            "Epoch 224/300\n",
            "13/13 [==============================] - 2s 129ms/step - loss: 2.1956e-04 - val_loss: 5.9238e-04\n",
            "Epoch 225/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 2.2910e-04 - val_loss: 4.6381e-04\n",
            "Epoch 226/300\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 2.3982e-04 - val_loss: 4.6057e-04\n",
            "Epoch 227/300\n",
            "13/13 [==============================] - 2s 144ms/step - loss: 2.2423e-04 - val_loss: 4.5772e-04\n",
            "Epoch 228/300\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 2.0567e-04 - val_loss: 4.7067e-04\n",
            "Epoch 229/300\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 2.4780e-04 - val_loss: 4.5820e-04\n",
            "Epoch 230/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.2250e-04 - val_loss: 4.5958e-04\n",
            "Epoch 231/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.3989e-04 - val_loss: 4.6963e-04\n",
            "Epoch 232/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.1424e-04 - val_loss: 4.6941e-04\n",
            "Epoch 233/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.6317e-04 - val_loss: 4.7433e-04\n",
            "Epoch 234/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 2.5128e-04 - val_loss: 4.3715e-04\n",
            "Epoch 235/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.5007e-04 - val_loss: 7.2149e-04\n",
            "Epoch 236/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 3.0883e-04 - val_loss: 4.7368e-04\n",
            "Epoch 237/300\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 3.2615e-04 - val_loss: 5.6281e-04\n",
            "Epoch 238/300\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 2.5464e-04 - val_loss: 4.5512e-04\n",
            "Epoch 239/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 2.1629e-04 - val_loss: 5.3140e-04\n",
            "Epoch 240/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.5543e-04 - val_loss: 4.8572e-04\n",
            "Epoch 241/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 2.7727e-04 - val_loss: 4.2627e-04\n",
            "Epoch 242/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.6284e-04 - val_loss: 4.6983e-04\n",
            "Epoch 243/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.3834e-04 - val_loss: 4.4712e-04\n",
            "Epoch 244/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.9316e-04 - val_loss: 4.7525e-04\n",
            "Epoch 245/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.4497e-04 - val_loss: 5.1857e-04\n",
            "Epoch 246/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 3.0610e-04 - val_loss: 4.9335e-04\n",
            "Epoch 247/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.8816e-04 - val_loss: 5.0867e-04\n",
            "Epoch 248/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.8477e-04 - val_loss: 4.3186e-04\n",
            "Epoch 249/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.5668e-04 - val_loss: 4.4299e-04\n",
            "Epoch 250/300\n",
            "13/13 [==============================] - 2s 127ms/step - loss: 2.3608e-04 - val_loss: 4.3020e-04\n",
            "Epoch 251/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.2556e-04 - val_loss: 4.5436e-04\n",
            "Epoch 252/300\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 2.0482e-04 - val_loss: 4.3939e-04\n",
            "Epoch 253/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 2.1836e-04 - val_loss: 4.3468e-04\n",
            "Epoch 254/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 2.6485e-04 - val_loss: 4.9256e-04\n",
            "Epoch 255/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 2.4369e-04 - val_loss: 5.1216e-04\n",
            "Epoch 256/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.3494e-04 - val_loss: 4.5391e-04\n",
            "Epoch 257/300\n",
            "13/13 [==============================] - 2s 126ms/step - loss: 2.3790e-04 - val_loss: 4.9038e-04\n",
            "Epoch 258/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.7322e-04 - val_loss: 4.0921e-04\n",
            "Epoch 259/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.2363e-04 - val_loss: 4.7321e-04\n",
            "Epoch 260/300\n",
            "13/13 [==============================] - 2s 116ms/step - loss: 2.4876e-04 - val_loss: 6.7151e-04\n",
            "Epoch 261/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.7617e-04 - val_loss: 5.7214e-04\n",
            "Epoch 262/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 3.3174e-04 - val_loss: 4.2539e-04\n",
            "Epoch 263/300\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 2.2677e-04 - val_loss: 4.1949e-04\n",
            "Epoch 264/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.3084e-04 - val_loss: 4.2961e-04\n",
            "Epoch 265/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.1239e-04 - val_loss: 4.1964e-04\n",
            "Epoch 266/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 1.9877e-04 - val_loss: 4.8714e-04\n",
            "Epoch 267/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.0216e-04 - val_loss: 4.0359e-04\n",
            "Epoch 268/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.0461e-04 - val_loss: 4.1686e-04\n",
            "Epoch 269/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.0320e-04 - val_loss: 4.5486e-04\n",
            "Epoch 270/300\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 1.9122e-04 - val_loss: 4.1218e-04\n",
            "Epoch 271/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.1537e-04 - val_loss: 4.0608e-04\n",
            "Epoch 272/300\n",
            "13/13 [==============================] - 2s 127ms/step - loss: 2.3716e-04 - val_loss: 6.9594e-04\n",
            "Epoch 273/300\n",
            "13/13 [==============================] - 2s 141ms/step - loss: 2.3155e-04 - val_loss: 4.9645e-04\n",
            "Epoch 274/300\n",
            "13/13 [==============================] - 2s 139ms/step - loss: 2.1622e-04 - val_loss: 4.1893e-04\n",
            "Epoch 275/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 1.9984e-04 - val_loss: 3.9810e-04\n",
            "Epoch 276/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 1.9798e-04 - val_loss: 4.0055e-04\n",
            "Epoch 277/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 1.9782e-04 - val_loss: 4.4314e-04\n",
            "Epoch 278/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 1.9450e-04 - val_loss: 4.3462e-04\n",
            "Epoch 279/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.3096e-04 - val_loss: 4.1334e-04\n",
            "Epoch 280/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 1.9099e-04 - val_loss: 4.1120e-04\n",
            "Epoch 281/300\n",
            "13/13 [==============================] - 2s 123ms/step - loss: 2.3886e-04 - val_loss: 4.5038e-04\n",
            "Epoch 282/300\n",
            "13/13 [==============================] - 2s 142ms/step - loss: 2.2679e-04 - val_loss: 3.9453e-04\n",
            "Epoch 283/300\n",
            "13/13 [==============================] - 2s 136ms/step - loss: 1.9925e-04 - val_loss: 3.9651e-04\n",
            "Epoch 284/300\n",
            "13/13 [==============================] - 2s 126ms/step - loss: 2.3188e-04 - val_loss: 4.6888e-04\n",
            "Epoch 285/300\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 2.1976e-04 - val_loss: 4.7980e-04\n",
            "Epoch 286/300\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 1.9946e-04 - val_loss: 3.9879e-04\n",
            "Epoch 287/300\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 2.0187e-04 - val_loss: 4.1726e-04\n",
            "Epoch 288/300\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 2.3391e-04 - val_loss: 3.9239e-04\n",
            "Epoch 289/300\n",
            "13/13 [==============================] - 2s 141ms/step - loss: 1.9768e-04 - val_loss: 3.9509e-04\n",
            "Epoch 290/300\n",
            "13/13 [==============================] - 2s 143ms/step - loss: 2.0448e-04 - val_loss: 3.9099e-04\n",
            "Epoch 291/300\n",
            "13/13 [==============================] - 2s 134ms/step - loss: 2.1206e-04 - val_loss: 4.6033e-04\n",
            "Epoch 292/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.0929e-04 - val_loss: 3.9261e-04\n",
            "Epoch 293/300\n",
            "13/13 [==============================] - 2s 128ms/step - loss: 2.0834e-04 - val_loss: 4.0184e-04\n",
            "Epoch 294/300\n",
            "13/13 [==============================] - 2s 119ms/step - loss: 2.2559e-04 - val_loss: 3.9626e-04\n",
            "Epoch 295/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.8645e-04 - val_loss: 4.0070e-04\n",
            "Epoch 296/300\n",
            "13/13 [==============================] - 2s 128ms/step - loss: 2.2223e-04 - val_loss: 4.7391e-04\n",
            "Epoch 297/300\n",
            "13/13 [==============================] - 2s 126ms/step - loss: 2.1226e-04 - val_loss: 5.8790e-04\n",
            "Epoch 298/300\n",
            "13/13 [==============================] - 2s 122ms/step - loss: 2.1932e-04 - val_loss: 3.9000e-04\n",
            "Epoch 299/300\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 1.9670e-04 - val_loss: 4.0206e-04\n",
            "Epoch 300/300\n",
            "13/13 [==============================] - 2s 144ms/step - loss: 1.9930e-04 - val_loss: 4.1191e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffafd187550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVcg6YfW3_t3"
      },
      "source": [
        "trainpredict = model.predict(xtrain)\n",
        "testpredict = model.predict(xtest)"
      ],
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyoUYg2GQH_w"
      },
      "source": [
        "# trainpredict = sc.inverse_transform(trainpredict)\n",
        "# testpredict = sc.inverse_transform(testpredict)"
      ],
      "execution_count": 421,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuhglG6rQcfG"
      },
      "source": [
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S09rQErKQo3K",
        "outputId": "1ed658c9-df00-40cc-c16d-8941c60b12da"
      },
      "source": [
        "sqrt(mean_squared_error(ytrain , trainpredict))"
      ],
      "execution_count": 423,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.013544434288142847"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 423
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFfygICMQpa-",
        "outputId": "6acd7aa6-b5ef-45d0-cc36-77a55538ac47"
      },
      "source": [
        "sqrt(mean_squared_error(ytest , testpredict))"
      ],
      "execution_count": 424,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02029554329869696"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 424
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL3NUeh_SDRF"
      },
      "source": [
        ""
      ],
      "execution_count": 424,
      "outputs": []
    }
  ]
}