{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stockprediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5efSpvoStIAM3lLHhSEE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alteregoxiv/stockpred/blob/main/stockprediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgd1YiGWppPz"
      },
      "source": [
        "# **STOCK PREDICTION USING LSTM(KERAS)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev5-W2HvWUpB"
      },
      "source": [
        "# **Importing necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT7zcAoIcksy"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVYQZhNWcKe"
      },
      "source": [
        "# **Training Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zxg_MYh3SOAk",
        "outputId": "293106bd-0076-485b-91eb-7808f19d7427"
      },
      "source": [
        "traindata = pd.read_csv(\"./AMZNtrain.csv\")\n",
        "traindata.head()"
      ],
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014-01-02</td>\n",
              "      <td>398.799988</td>\n",
              "      <td>399.359985</td>\n",
              "      <td>394.019989</td>\n",
              "      <td>397.970001</td>\n",
              "      <td>397.970001</td>\n",
              "      <td>2137800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014-01-03</td>\n",
              "      <td>398.290009</td>\n",
              "      <td>402.709991</td>\n",
              "      <td>396.220001</td>\n",
              "      <td>396.440002</td>\n",
              "      <td>396.440002</td>\n",
              "      <td>2210200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014-01-06</td>\n",
              "      <td>395.850006</td>\n",
              "      <td>397.000000</td>\n",
              "      <td>388.420013</td>\n",
              "      <td>393.630005</td>\n",
              "      <td>393.630005</td>\n",
              "      <td>3170600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2014-01-07</td>\n",
              "      <td>395.040009</td>\n",
              "      <td>398.470001</td>\n",
              "      <td>394.290009</td>\n",
              "      <td>398.029999</td>\n",
              "      <td>398.029999</td>\n",
              "      <td>1916000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014-01-08</td>\n",
              "      <td>398.470001</td>\n",
              "      <td>403.000000</td>\n",
              "      <td>396.040009</td>\n",
              "      <td>401.920013</td>\n",
              "      <td>401.920013</td>\n",
              "      <td>2316500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date        Open        High  ...       Close   Adj Close   Volume\n",
              "0  2014-01-02  398.799988  399.359985  ...  397.970001  397.970001  2137800\n",
              "1  2014-01-03  398.290009  402.709991  ...  396.440002  396.440002  2210200\n",
              "2  2014-01-06  395.850006  397.000000  ...  393.630005  393.630005  3170600\n",
              "3  2014-01-07  395.040009  398.470001  ...  398.029999  398.029999  1916000\n",
              "4  2014-01-08  398.470001  403.000000  ...  401.920013  401.920013  2316500\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 380
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPEpZlynWnQW"
      },
      "source": [
        "# **Testing Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JVYD_bNkSg5d",
        "outputId": "63a73572-e283-49f3-d157-3b1de6639b5d"
      },
      "source": [
        "testdata = pd.read_csv(\"./AMZNtest.csv\")\n",
        "testdata.head()"
      ],
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-01-02</td>\n",
              "      <td>1465.199951</td>\n",
              "      <td>1553.359985</td>\n",
              "      <td>1460.930054</td>\n",
              "      <td>1539.130005</td>\n",
              "      <td>1539.130005</td>\n",
              "      <td>7983100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-01-03</td>\n",
              "      <td>1520.010010</td>\n",
              "      <td>1538.000000</td>\n",
              "      <td>1497.109985</td>\n",
              "      <td>1500.280029</td>\n",
              "      <td>1500.280029</td>\n",
              "      <td>6975600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-04</td>\n",
              "      <td>1530.000000</td>\n",
              "      <td>1594.000000</td>\n",
              "      <td>1518.310059</td>\n",
              "      <td>1575.390015</td>\n",
              "      <td>1575.390015</td>\n",
              "      <td>9182600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-01-07</td>\n",
              "      <td>1602.310059</td>\n",
              "      <td>1634.560059</td>\n",
              "      <td>1589.189941</td>\n",
              "      <td>1629.510010</td>\n",
              "      <td>1629.510010</td>\n",
              "      <td>7993200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-01-08</td>\n",
              "      <td>1664.689941</td>\n",
              "      <td>1676.609985</td>\n",
              "      <td>1616.609985</td>\n",
              "      <td>1656.579956</td>\n",
              "      <td>1656.579956</td>\n",
              "      <td>8881400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date         Open         High  ...        Close    Adj Close   Volume\n",
              "0  2019-01-02  1465.199951  1553.359985  ...  1539.130005  1539.130005  7983100\n",
              "1  2019-01-03  1520.010010  1538.000000  ...  1500.280029  1500.280029  6975600\n",
              "2  2019-01-04  1530.000000  1594.000000  ...  1575.390015  1575.390015  9182600\n",
              "3  2019-01-07  1602.310059  1634.560059  ...  1629.510010  1629.510010  7993200\n",
              "4  2019-01-08  1664.689941  1676.609985  ...  1656.579956  1656.579956  8881400\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URHQm4XdWrNU"
      },
      "source": [
        "# **Shapes of both data sets(Train , Test)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF6nBNrZU7Ux",
        "outputId": "8e220fe9-552c-4c3b-b805-9b15116781b4"
      },
      "source": [
        "traindata.shape , testdata.shape"
      ],
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1258, 7), (21, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZOISlTXUqo"
      },
      "source": [
        "# **Concatenating Datasers and Sorting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMCJ0YqEVVlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430b4d76-5e89-4388-f046-b555f5bdb3bf"
      },
      "source": [
        "data = pd.concat([traindata , testdata] , axis=0 , ignore_index=True)\n",
        "data.head"
      ],
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of             Date         Open         High  ...        Close    Adj Close    Volume\n",
              "0     2014-01-02   398.799988   399.359985  ...   397.970001   397.970001   2137800\n",
              "1     2014-01-03   398.290009   402.709991  ...   396.440002   396.440002   2210200\n",
              "2     2014-01-06   395.850006   397.000000  ...   393.630005   393.630005   3170600\n",
              "3     2014-01-07   395.040009   398.470001  ...   398.029999   398.029999   1916000\n",
              "4     2014-01-08   398.470001   403.000000  ...   401.920013   401.920013   2316500\n",
              "...          ...          ...          ...  ...          ...          ...       ...\n",
              "1274  2019-01-25  1670.500000  1683.479980  ...  1670.569946  1670.569946   4945900\n",
              "1275  2019-01-28  1643.589966  1645.000000  ...  1637.890015  1637.890015   4837700\n",
              "1276  2019-01-29  1631.270020  1632.380005  ...  1593.880005  1593.880005   4632800\n",
              "1277  2019-01-30  1623.000000  1676.949951  ...  1670.430054  1670.430054   5783800\n",
              "1278  2019-01-31  1692.849976  1736.410034  ...  1718.729980  1718.729980  10910300\n",
              "\n",
              "[1279 rows x 7 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 383
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "SEFBWlNzXazd",
        "outputId": "8d5ae719-5705-4636-8078-5a9aa9acd1e7"
      },
      "source": [
        "for i in range(len(data)):\n",
        "  date = data[\"Date\"][i]\n",
        "  d = date.split(\"-\")\n",
        "  s = str(d[0]) + str(d[1]) + str(d[2])\n",
        "  data[\"Date\"][i] = s\n",
        "data.head()"
      ],
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20140102</td>\n",
              "      <td>398.799988</td>\n",
              "      <td>399.359985</td>\n",
              "      <td>394.019989</td>\n",
              "      <td>397.970001</td>\n",
              "      <td>397.970001</td>\n",
              "      <td>2137800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20140103</td>\n",
              "      <td>398.290009</td>\n",
              "      <td>402.709991</td>\n",
              "      <td>396.220001</td>\n",
              "      <td>396.440002</td>\n",
              "      <td>396.440002</td>\n",
              "      <td>2210200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20140106</td>\n",
              "      <td>395.850006</td>\n",
              "      <td>397.000000</td>\n",
              "      <td>388.420013</td>\n",
              "      <td>393.630005</td>\n",
              "      <td>393.630005</td>\n",
              "      <td>3170600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20140107</td>\n",
              "      <td>395.040009</td>\n",
              "      <td>398.470001</td>\n",
              "      <td>394.290009</td>\n",
              "      <td>398.029999</td>\n",
              "      <td>398.029999</td>\n",
              "      <td>1916000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20140108</td>\n",
              "      <td>398.470001</td>\n",
              "      <td>403.000000</td>\n",
              "      <td>396.040009</td>\n",
              "      <td>401.920013</td>\n",
              "      <td>401.920013</td>\n",
              "      <td>2316500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Date        Open        High  ...       Close   Adj Close   Volume\n",
              "0  20140102  398.799988  399.359985  ...  397.970001  397.970001  2137800\n",
              "1  20140103  398.290009  402.709991  ...  396.440002  396.440002  2210200\n",
              "2  20140106  395.850006  397.000000  ...  393.630005  393.630005  3170600\n",
              "3  20140107  395.040009  398.470001  ...  398.029999  398.029999  1916000\n",
              "4  20140108  398.470001  403.000000  ...  401.920013  401.920013  2316500\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 384
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl95h11OhUXG",
        "outputId": "794be2fa-f74a-4cb6-b9a2-22a0bad9e9b9"
      },
      "source": [
        "data = data.sort_index(ascending=True , axis=0)\n",
        "data.head"
      ],
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of           Date         Open         High  ...        Close    Adj Close    Volume\n",
              "0     20140102   398.799988   399.359985  ...   397.970001   397.970001   2137800\n",
              "1     20140103   398.290009   402.709991  ...   396.440002   396.440002   2210200\n",
              "2     20140106   395.850006   397.000000  ...   393.630005   393.630005   3170600\n",
              "3     20140107   395.040009   398.470001  ...   398.029999   398.029999   1916000\n",
              "4     20140108   398.470001   403.000000  ...   401.920013   401.920013   2316500\n",
              "...        ...          ...          ...  ...          ...          ...       ...\n",
              "1274  20190125  1670.500000  1683.479980  ...  1670.569946  1670.569946   4945900\n",
              "1275  20190128  1643.589966  1645.000000  ...  1637.890015  1637.890015   4837700\n",
              "1276  20190129  1631.270020  1632.380005  ...  1593.880005  1593.880005   4632800\n",
              "1277  20190130  1623.000000  1676.949951  ...  1670.430054  1670.430054   5783800\n",
              "1278  20190131  1692.849976  1736.410034  ...  1718.729980  1718.729980  10910300\n",
              "\n",
              "[1279 rows x 7 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 385
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmL2H46GhxzR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFvm1e-Vnsvt"
      },
      "source": [
        "# **Saving the DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBruMR6WhxB7"
      },
      "source": [
        "data.to_csv(r'./AMZNTotalData.csv' , index=False)"
      ],
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvQ5cI7ykyvr",
        "outputId": "7e4052ad-5eec-43e2-a865-3164c0c80cae"
      },
      "source": [
        "newdata = pd.read_csv(\"./AMZNTotalData.csv\")\n",
        "newdata.head"
      ],
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of           Date         Open         High  ...        Close    Adj Close    Volume\n",
              "0     20140102   398.799988   399.359985  ...   397.970001   397.970001   2137800\n",
              "1     20140103   398.290009   402.709991  ...   396.440002   396.440002   2210200\n",
              "2     20140106   395.850006   397.000000  ...   393.630005   393.630005   3170600\n",
              "3     20140107   395.040009   398.470001  ...   398.029999   398.029999   1916000\n",
              "4     20140108   398.470001   403.000000  ...   401.920013   401.920013   2316500\n",
              "...        ...          ...          ...  ...          ...          ...       ...\n",
              "1274  20190125  1670.500000  1683.479980  ...  1670.569946  1670.569946   4945900\n",
              "1275  20190128  1643.589966  1645.000000  ...  1637.890015  1637.890015   4837700\n",
              "1276  20190129  1631.270020  1632.380005  ...  1593.880005  1593.880005   4632800\n",
              "1277  20190130  1623.000000  1676.949951  ...  1670.430054  1670.430054   5783800\n",
              "1278  20190131  1692.849976  1736.410034  ...  1718.729980  1718.729980  10910300\n",
              "\n",
              "[1279 rows x 7 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 387
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37LGB3wEp93X"
      },
      "source": [
        "# **Test Train Split from the concatenated Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZnXpPuIonv9",
        "outputId": "c3f4b65d-1747-4f0c-8811-15b7f0970e4d"
      },
      "source": [
        "limit = np.random.rand((len(newdata)))<0.7\n",
        "train = newdata[limit]\n",
        "test = newdata[~limit]\n",
        "train.shape , test.shape"
      ],
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((907, 7), (372, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXTXV-D4qQot",
        "outputId": "3a12a38b-9e3f-40bc-9401-51088d42c02f"
      },
      "source": [
        "newtrain = train[['Adj Close']]\n",
        "newtest = test[['Adj Close']]\n",
        "\n",
        "newtrain.shape , newtest.shape"
      ],
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((907, 1), (372, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 389
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD7YgMi7yU3g"
      },
      "source": [
        "# **Scaling the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5e02dL5-PoF"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "newtrain = sc.fit_transform(newtrain)\n",
        "newtest = sc.fit_transform(newtest)\n",
        "\n",
        "newtrain = np.reshape(newtrain , (-1 , 1))\n",
        "newtest = np.reshape(newtest , (-1 , 1))"
      ],
      "execution_count": 390,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N84K9DW2yepY"
      },
      "source": [
        "# **Preparing the Final data using timestep for LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAixTgSxeNNF"
      },
      "source": [
        "def final_data(data , timestep):\n",
        "  xdata , ydata = [] , []\n",
        "  l = len(data)\n",
        "  for i in range(l-timestep):\n",
        "    a = data[i : i+timestep , 0]\n",
        "    xdata.append(a)\n",
        "    ydata.append(data[i+timestep , 0])\n",
        "  return np.array(xdata) , np.array(ydata)\n",
        "\n",
        "\n",
        "\n",
        "timestep = 60\n",
        "xtrain  , ytrain = final_data(newtrain , timestep)\n",
        "xtest , ytest = final_data(newtest , timestep)"
      ],
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1JwMQYh5GZ"
      },
      "source": [
        "xtrain = xtrain.reshape(xtrain.shape[0] , xtrain.shape[1] , 1)\n",
        "xtest = xtest.reshape(xtest.shape[0] , xtest.shape[1] , 1)\n",
        "#xtrain.shape\n",
        "#print(xtrain)"
      ],
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G-BglZkzh6V"
      },
      "source": [
        "# **Preparing Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC_0T-jFjw6E"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , LSTM"
      ],
      "execution_count": 393,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra1u6nyUkkVV"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50 , return_sequences=True , input_shape=(60 , 1)))\n",
        "model.add(LSTM(50 , return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error' , optimizer='adam')"
      ],
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWzEihXDnLmu",
        "outputId": "ac07fefc-a3bc-4e8f-d79b-d8791b8a2d9c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_47 (LSTM)               (None, 60, 50)            10400     \n",
            "_________________________________________________________________\n",
            "lstm_48 (LSTM)               (None, 60, 50)            20200     \n",
            "_________________________________________________________________\n",
            "lstm_49 (LSTM)               (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 50,851\n",
            "Trainable params: 50,851\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GUCkn8pzmYy"
      },
      "source": [
        "# **Fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IESruTGgnRog",
        "outputId": "6030e860-ac8a-424e-8509-ce889488efff"
      },
      "source": [
        "model.fit(xtrain , ytrain , validation_data=(xtest , ytest) , epochs=200 , batch_size=64 , verbose=1)"
      ],
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 8s 261ms/step - loss: 0.0493 - val_loss: 0.0189\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.0079 - val_loss: 0.0045\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0025 - val_loss: 0.0037\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 0.0014 - val_loss: 0.0033\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0011 - val_loss: 0.0033\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 0.0012 - val_loss: 0.0031\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0012 - val_loss: 0.0032\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0011 - val_loss: 0.0029\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 0.0010 - val_loss: 0.0028\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 2s 111ms/step - loss: 0.0010 - val_loss: 0.0034\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 2s 111ms/step - loss: 0.0011 - val_loss: 0.0028\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 0.0011 - val_loss: 0.0032\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 0.0010 - val_loss: 0.0027\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 2s 124ms/step - loss: 0.0012 - val_loss: 0.0030\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 0.0010 - val_loss: 0.0025\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 0.0011 - val_loss: 0.0030\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 9.2695e-04 - val_loss: 0.0025\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 8.6458e-04 - val_loss: 0.0025\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 8.3915e-04 - val_loss: 0.0023\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 8.1786e-04 - val_loss: 0.0023\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 8.1263e-04 - val_loss: 0.0023\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 7.9748e-04 - val_loss: 0.0022\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 2s 121ms/step - loss: 8.1035e-04 - val_loss: 0.0022\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 7.9397e-04 - val_loss: 0.0022\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 7.8901e-04 - val_loss: 0.0021\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 8.4592e-04 - val_loss: 0.0022\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 8.1484e-04 - val_loss: 0.0022\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 9.6603e-04 - val_loss: 0.0021\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 8.3669e-04 - val_loss: 0.0021\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 7.3365e-04 - val_loss: 0.0019\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 7.6330e-04 - val_loss: 0.0019\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 7.3496e-04 - val_loss: 0.0019\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 2s 112ms/step - loss: 7.2002e-04 - val_loss: 0.0020\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 9.3875e-04 - val_loss: 0.0020\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 2s 124ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 7.6589e-04 - val_loss: 0.0018\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 6.8756e-04 - val_loss: 0.0019\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 7.9783e-04 - val_loss: 0.0018\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 7.2514e-04 - val_loss: 0.0017\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 7.3488e-04 - val_loss: 0.0019\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 7.7753e-04 - val_loss: 0.0017\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 7.0304e-04 - val_loss: 0.0017\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 7.7497e-04 - val_loss: 0.0019\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 8.4304e-04 - val_loss: 0.0018\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 2s 123ms/step - loss: 7.0270e-04 - val_loss: 0.0016\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 6.6833e-04 - val_loss: 0.0018\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 2s 122ms/step - loss: 6.8819e-04 - val_loss: 0.0016\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 6.3159e-04 - val_loss: 0.0015\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 6.2058e-04 - val_loss: 0.0016\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 7.3019e-04 - val_loss: 0.0016\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 6.7132e-04 - val_loss: 0.0021\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 7.5457e-04 - val_loss: 0.0015\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 5.9885e-04 - val_loss: 0.0016\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 7.0965e-04 - val_loss: 0.0017\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 2s 123ms/step - loss: 6.9980e-04 - val_loss: 0.0015\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 6.7315e-04 - val_loss: 0.0016\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 6.1166e-04 - val_loss: 0.0017\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 5.9707e-04 - val_loss: 0.0013\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 2s 129ms/step - loss: 5.6153e-04 - val_loss: 0.0014\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 2s 125ms/step - loss: 5.6727e-04 - val_loss: 0.0013\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 6.6457e-04 - val_loss: 0.0017\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 7.0118e-04 - val_loss: 0.0015\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 8.1848e-04 - val_loss: 0.0013\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 6.7967e-04 - val_loss: 0.0019\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 6.8283e-04 - val_loss: 0.0012\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 6.6722e-04 - val_loss: 0.0012\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 5.7883e-04 - val_loss: 0.0013\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 5.0112e-04 - val_loss: 0.0012\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 5.0873e-04 - val_loss: 0.0011\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 4.9122e-04 - val_loss: 0.0013\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 6.3452e-04 - val_loss: 0.0011\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 5.1872e-04 - val_loss: 0.0012\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 5.5083e-04 - val_loss: 0.0013\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 4.6565e-04 - val_loss: 0.0011\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 4.8742e-04 - val_loss: 0.0012\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 4.4928e-04 - val_loss: 0.0011\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 4.0715e-04 - val_loss: 0.0010\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 4.0243e-04 - val_loss: 0.0011\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 4.1150e-04 - val_loss: 0.0010\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 3.8956e-04 - val_loss: 0.0010\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 4.0656e-04 - val_loss: 0.0011\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 4.3092e-04 - val_loss: 9.5650e-04\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 3.6308e-04 - val_loss: 9.3302e-04\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 3.6553e-04 - val_loss: 0.0011\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 2s 126ms/step - loss: 3.4453e-04 - val_loss: 9.9661e-04\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 2s 124ms/step - loss: 3.3476e-04 - val_loss: 8.6654e-04\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 3.5476e-04 - val_loss: 9.3527e-04\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 3.8408e-04 - val_loss: 0.0011\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 5.0103e-04 - val_loss: 0.0010\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 4.1520e-04 - val_loss: 8.7538e-04\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 3.9843e-04 - val_loss: 0.0010\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 2s 129ms/step - loss: 3.3955e-04 - val_loss: 8.9556e-04\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 3.1185e-04 - val_loss: 8.4951e-04\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 3.0619e-04 - val_loss: 8.7470e-04\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 2.9719e-04 - val_loss: 9.2828e-04\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 3.0410e-04 - val_loss: 9.0310e-04\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 2s 126ms/step - loss: 3.2575e-04 - val_loss: 8.5882e-04\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 2s 122ms/step - loss: 2.9013e-04 - val_loss: 8.1961e-04\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 2s 124ms/step - loss: 2.9893e-04 - val_loss: 8.6125e-04\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 3.2810e-04 - val_loss: 8.2357e-04\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 3.1326e-04 - val_loss: 8.3972e-04\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 2.9224e-04 - val_loss: 8.3691e-04\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.7639e-04 - val_loss: 8.7009e-04\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.8913e-04 - val_loss: 8.0540e-04\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.8132e-04 - val_loss: 7.8259e-04\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 3.0059e-04 - val_loss: 7.8630e-04\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 4.1923e-04 - val_loss: 8.5167e-04\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 2s 121ms/step - loss: 3.6182e-04 - val_loss: 8.0842e-04\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 2s 130ms/step - loss: 2.8344e-04 - val_loss: 8.0900e-04\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 2s 127ms/step - loss: 3.5984e-04 - val_loss: 8.7334e-04\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 3.7016e-04 - val_loss: 7.6716e-04\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 3.4389e-04 - val_loss: 8.8405e-04\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 3.3168e-04 - val_loss: 8.2611e-04\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 3.3752e-04 - val_loss: 7.8801e-04\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.7484e-04 - val_loss: 8.2329e-04\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.8741e-04 - val_loss: 7.4848e-04\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 2s 123ms/step - loss: 2.8754e-04 - val_loss: 8.0038e-04\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.7213e-04 - val_loss: 8.7504e-04\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 2s 122ms/step - loss: 2.8340e-04 - val_loss: 8.3868e-04\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.8885e-04 - val_loss: 7.6221e-04\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.7841e-04 - val_loss: 7.5898e-04\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 2s 124ms/step - loss: 3.3464e-04 - val_loss: 7.7686e-04\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 3.0115e-04 - val_loss: 7.6418e-04\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 3.2520e-04 - val_loss: 7.3887e-04\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 2s 125ms/step - loss: 2.8640e-04 - val_loss: 8.5198e-04\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 2.9039e-04 - val_loss: 8.0287e-04\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 2s 127ms/step - loss: 3.4076e-04 - val_loss: 9.4511e-04\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 2s 124ms/step - loss: 2.6873e-04 - val_loss: 7.2965e-04\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.7062e-04 - val_loss: 7.2358e-04\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.4501e-04 - val_loss: 7.5883e-04\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.5909e-04 - val_loss: 7.4447e-04\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 3.3547e-04 - val_loss: 7.0991e-04\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.6483e-04 - val_loss: 7.0068e-04\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 2.5590e-04 - val_loss: 7.1434e-04\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 2s 123ms/step - loss: 2.7498e-04 - val_loss: 7.1652e-04\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 2s 123ms/step - loss: 2.8250e-04 - val_loss: 7.1648e-04\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 2.8430e-04 - val_loss: 7.2624e-04\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 2s 123ms/step - loss: 3.6250e-04 - val_loss: 8.3713e-04\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 4.8187e-04 - val_loss: 8.8956e-04\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.9924e-04 - val_loss: 7.0854e-04\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 2s 126ms/step - loss: 2.5553e-04 - val_loss: 7.0534e-04\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 2s 113ms/step - loss: 2.4703e-04 - val_loss: 7.0289e-04\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 2s 121ms/step - loss: 2.4833e-04 - val_loss: 8.3576e-04\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 2.5592e-04 - val_loss: 7.2473e-04\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 2.7887e-04 - val_loss: 7.3295e-04\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.4810e-04 - val_loss: 7.8603e-04\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 2s 121ms/step - loss: 2.6710e-04 - val_loss: 7.0433e-04\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 2.5067e-04 - val_loss: 6.6800e-04\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 2.7063e-04 - val_loss: 6.8224e-04\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 2.6808e-04 - val_loss: 7.5866e-04\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.4744e-04 - val_loss: 8.6614e-04\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.7230e-04 - val_loss: 7.2842e-04\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 2.5899e-04 - val_loss: 6.6561e-04\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 2s 128ms/step - loss: 2.3805e-04 - val_loss: 7.1934e-04\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.4664e-04 - val_loss: 6.6300e-04\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 2.4463e-04 - val_loss: 6.7538e-04\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 2s 114ms/step - loss: 2.5718e-04 - val_loss: 7.3064e-04\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 2.9820e-04 - val_loss: 9.4586e-04\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.9657e-04 - val_loss: 7.2779e-04\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.5696e-04 - val_loss: 8.5485e-04\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 2s 120ms/step - loss: 2.7342e-04 - val_loss: 8.9918e-04\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 2.7398e-04 - val_loss: 7.3951e-04\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 2.5739e-04 - val_loss: 6.9244e-04\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.4727e-04 - val_loss: 7.0134e-04\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 2.6655e-04 - val_loss: 6.6543e-04\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 2s 122ms/step - loss: 2.9663e-04 - val_loss: 6.3332e-04\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 3.8698e-04 - val_loss: 6.2398e-04\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.8865e-04 - val_loss: 7.1572e-04\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 2.7926e-04 - val_loss: 9.9116e-04\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 3.2371e-04 - val_loss: 6.3876e-04\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.2464e-04 - val_loss: 6.2854e-04\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.3529e-04 - val_loss: 7.0401e-04\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 3.4745e-04 - val_loss: 6.5969e-04\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.4463e-04 - val_loss: 6.5535e-04\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 2.7307e-04 - val_loss: 6.4764e-04\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 2s 123ms/step - loss: 3.0464e-04 - val_loss: 6.8161e-04\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 2.4884e-04 - val_loss: 6.2468e-04\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 2.5302e-04 - val_loss: 7.1157e-04\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 3.0198e-04 - val_loss: 6.8024e-04\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 4.0390e-04 - val_loss: 6.2447e-04\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 3.0795e-04 - val_loss: 6.3053e-04\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 2s 118ms/step - loss: 2.5971e-04 - val_loss: 7.1566e-04\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.2733e-04 - val_loss: 6.1940e-04\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.7516e-04 - val_loss: 6.6276e-04\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 2.9858e-04 - val_loss: 8.2378e-04\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 3.4555e-04 - val_loss: 6.2693e-04\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.2468e-04 - val_loss: 6.4564e-04\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 2s 117ms/step - loss: 2.3091e-04 - val_loss: 6.1198e-04\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 2s 116ms/step - loss: 2.1915e-04 - val_loss: 6.0067e-04\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 2s 119ms/step - loss: 2.1983e-04 - val_loss: 6.0848e-04\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 2s 115ms/step - loss: 2.3294e-04 - val_loss: 6.5047e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffaf782cd50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 396
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVcg6YfW3_t3"
      },
      "source": [
        "trainpredict = model.predict(xtrain)\n",
        "testpredict = model.predict(xtest)"
      ],
      "execution_count": 397,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyoUYg2GQH_w"
      },
      "source": [
        "# trainpredict = sc.inverse_transform(trainpredict)\n",
        "# testpredict = sc.inverse_transform(testpredict)"
      ],
      "execution_count": 398,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuhglG6rQcfG"
      },
      "source": [
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S09rQErKQo3K",
        "outputId": "e5d25e98-411e-4ffc-9670-451420b02bc3"
      },
      "source": [
        "sqrt(mean_squared_error(ytrain , trainpredict))"
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0160894457606694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 400
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFfygICMQpa-",
        "outputId": "c85e8137-8903-440e-de0d-eec4e0239f0c"
      },
      "source": [
        "sqrt(mean_squared_error(ytest , testpredict))"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02550430263730504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 401
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL3NUeh_SDRF"
      },
      "source": [
        ""
      ],
      "execution_count": 401,
      "outputs": []
    }
  ]
}